{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c0949f",
   "metadata": {},
   "source": [
    "### Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ca3efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import plot_confusion_matrix, auc, roc_curve, plot_roc_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372e8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'/Users/benshi/Downloads/default_data.xls').drop([0])\n",
    "df = df.iloc[:,1:]\n",
    "\n",
    "# Specify target and covariate data\n",
    "# Using the most relevant X predictors calculated in HW 1, since it took WAY too long to run SVM on the full \n",
    "# dataset\n",
    "X = df[['X6', 'X7', 'X8', 'X9', 'X10']]\n",
    "y = df['Y']\n",
    "\n",
    "# Replace missing with column mean\n",
    "X = X.fillna(X.mean()) \n",
    "\n",
    "# Clip the data at 2% tails\n",
    "X = X.clip(lower=X.quantile(0.02), upper=X.quantile(0.98), axis=1)\n",
    "\n",
    "# Normalizing scale\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Converting to float:\n",
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_test = y_test.astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd0aa77",
   "metadata": {},
   "source": [
    "(a) (SVM). This part of the question asks you to use the SVM approach to predict default. For all performance evaluation, you should start by leaving out 20% of your data for testing, and report the out-of-sample metrics (e.g. ROC AUC):\n",
    "(i) Fit the SVM using linear, Gaussian (RBF in sklearn) and polynomial kernals. Compare the ROC AUC for the training data and the ROC AUC area for the cross validation (test) data. Provide some intuition for your results, especially on how different kernals perform differently. (HINT: For this step, you should change γ and C while fixing the kernel to be ‘linear’, ‘rbf’, and ‘poly’ each time. The idea is that you’re finding the “best” linear (poly, rbf) SVM.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e643e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Score for linear kernel:   0.686750344194014\n",
      "Best Params for linear kernel:  {'C': 1, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'linear', 'max_iter': 100000000.0}\n",
      "Best Score for polynomial kernel:   0.7026371552833273\n",
      "Best Params for polynomial kernel:  {'C': 10, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'poly', 'max_iter': 100000000.0}\n",
      "Best Score for rbf kernel:   0.7058645200368978\n",
      "Best Params for rbf kernel:  {'C': 1, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': 100000000.0}\n",
      "ROC AUC area for test data, linear kernel: 0.701\n",
      "ROC AUC area for test data, polynomial kernel: 0.707\n",
      "ROC AUC area for test data, rbf kernel: 0.725\n"
     ]
    }
   ],
   "source": [
    "param_grid_lin = dict(C=[1, 10],\n",
    "                  gamma = [0.001, 0.01, 0.1],\n",
    "                  kernel=['linear'],\n",
    "                  max_iter=[1e8],\n",
    "                  class_weight=['balanced'])\n",
    "param_grid_poly = dict(C=[1, 10],\n",
    "                  gamma = [0.001, 0.01, 0.1],\n",
    "                  kernel=['poly'],\n",
    "                  max_iter=[1e8],\n",
    "                  class_weight=['balanced'])\n",
    "param_grid_rbf = dict(C=[1, 10],\n",
    "                  gamma = [0.001, 0.01, 0.1],\n",
    "                  kernel=['rbf'],\n",
    "                  max_iter=[1e8],\n",
    "                  class_weight=['balanced'])\n",
    "\n",
    "# Define classifier\n",
    "clf_svm = SVC()\n",
    "# Scoring criterion\n",
    "roc = make_scorer(roc_auc_score)\n",
    "\n",
    "# Searching for best with grid search\n",
    "grid_lin = GridSearchCV(estimator=clf_svm,\n",
    "                    param_grid=param_grid_lin,\n",
    "                    scoring=roc,\n",
    "                    verbose=1,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1)\n",
    "grid_poly = GridSearchCV(estimator=clf_svm,\n",
    "                    param_grid=param_grid_poly,\n",
    "                    scoring=roc,\n",
    "                    verbose=1,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1)\n",
    "grid_rbf = GridSearchCV(estimator=clf_svm,\n",
    "                    param_grid=param_grid_rbf,\n",
    "                    scoring=roc,\n",
    "                    verbose=1,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid_result_svm_lin = grid_lin.fit(X_train, y_train)\n",
    "grid_result_svm_poly = grid_poly.fit(X_train, y_train)\n",
    "grid_result_svm_rbf = grid_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Printing them all out\n",
    "print('Best Score for linear kernel:  ', grid_result_svm_lin.best_score_)\n",
    "print('Best Params for linear kernel: ', grid_result_svm_lin.best_params_)\n",
    "print('Best Score for polynomial kernel:  ', grid_result_svm_poly.best_score_)\n",
    "print('Best Params for polynomial kernel: ', grid_result_svm_poly.best_params_)\n",
    "print('Best Score for rbf kernel:  ', grid_result_svm_rbf.best_score_)\n",
    "print('Best Params for rbf kernel: ', grid_result_svm_rbf.best_params_)\n",
    "\n",
    "# Fitting on test data \n",
    "clf_svm_lin = SVC(**grid_result_svm_lin.best_params_).fit(X_train, y_train)\n",
    "clf_svm_poly = SVC(**grid_result_svm_poly.best_params_).fit(X_train, y_train)\n",
    "clf_svm_rbf = SVC(**grid_result_svm_rbf.best_params_).fit(X_train, y_train)\n",
    "\n",
    "y_test_score_lin = clf_svm_lin.decision_function(X_test)\n",
    "y_test_score_poly = clf_svm_poly.decision_function(X_test)\n",
    "y_test_score_rbf = clf_svm_rbf.decision_function(X_test)\n",
    "\n",
    "fpr_svm_lin, tpr_svm_lin, _ = roc_curve(y_test, y_test_score_lin)\n",
    "fpr_svm_poly, tpr_svm_poly, _ = roc_curve(y_test, y_test_score_poly)\n",
    "fpr_svm_rbf, tpr_svm_rbf, _ = roc_curve(y_test, y_test_score_rbf)\n",
    "\n",
    "auc_svm_lin = auc(fpr_svm_lin, tpr_svm_lin)\n",
    "auc_svm_poly = auc(fpr_svm_poly, tpr_svm_poly)\n",
    "auc_svm_rbf = auc(fpr_svm_rbf, tpr_svm_rbf)\n",
    "\n",
    "print('ROC AUC area for test data, linear kernel: {:.3f}'.format(auc_svm_lin))\n",
    "print('ROC AUC area for test data, polynomial kernel: {:.3f}'.format(auc_svm_poly))\n",
    "print('ROC AUC area for test data, rbf kernel: {:.3f}'.format(auc_svm_rbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e157cacc",
   "metadata": {},
   "source": [
    "Looking at the results here, it seems that the rbf kernel performed best, with an ROC AUC area of 0.725 for test data. This makes relative sense, since the rbf kernel forms circular decision boundaries around data, so this would logically make it optimal for classification problems involving data that separate into well defined clusters. The linear kernel performed the worse, which makes sense, since it partitions space with a hyperplane, which is the most \"undynamic\" type of decision boundary. The polynomial performs in the middle: which is somewhat expected while unexpected, since it could potentially outperform the rbf in certain data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df52ac27",
   "metadata": {},
   "source": [
    "(ii) Based on the results from the previous step, try to optimize the classification perfor- mance of the SVM by choosing whatever parameters you could choose, and provide some rationale for these choices. Report your “optimized” ROC AUC on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b5915c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "Best Score:   0.7067827879962439\n",
      "Best Params:  {'C': 5, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': 100000000.0}\n",
      "Optimzed ROC AUC area for test data: 0.706\n"
     ]
    }
   ],
   "source": [
    "# Rationale for these choices of parameters: As I ran the grid search over and over again, it seemed that the gamma\n",
    "# value converged to an optimal value of 0.1 (0.1 was chosen regardless of the other candidates), and the C value\n",
    "# converged to an optimal value of 5, so then I narrowed the options down to the numbers closest to the converged values.\n",
    "# Grid search then continued to say that 5 and 0.1 were optimal values, so it must be optimal.\n",
    "param_grid = dict(C=[1, 4, 5, 6, 7],\n",
    "                  kernel=['rbf'],\n",
    "                  gamma = [0.05, 0.1, 0.15], \n",
    "                  max_iter=[1e8],\n",
    "                  class_weight=['balanced'])\n",
    "\n",
    "# Define classifier\n",
    "clf_svm = SVC()\n",
    "# Scoring criterion\n",
    "roc = make_scorer(roc_auc_score)\n",
    "\n",
    "# Searching for best with grid search\n",
    "grid = GridSearchCV(estimator=clf_svm,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring=roc,\n",
    "                    verbose=1,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid_result_svm = grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score:  ', grid_result_svm.best_score_)\n",
    "print('Best Params: ', grid_result_svm.best_params_)\n",
    "\n",
    "# Fitting on test data \n",
    "clf_svm = SVC(**grid_result_svm.best_params_).fit(X_train, y_train)\n",
    "y_test_score = clf_svm.decision_function(X_test)\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test, y_test_score)\n",
    "auc_svm = auc(fpr_svm, tpr_svm)\n",
    "print('Optimzed ROC AUC area for test data: {:.3f}'.format(auc_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaff297",
   "metadata": {},
   "source": [
    "(b) (Decision trees). For this part, you need to use a decision tree to predict default:\n",
    "(i) Grow a collection of trees by varying the maximum tree depth and the pruning param- eter. Try the grid search done in precept to see what combination seems to work well. For each tree, report accuracy (you should be using sklearn.tree.DecisionTreeClassifier for now rather than the random forest). Provide some intuition for your results. (E.g. why some trees appear to work better.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c6f28a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is: 0.8224583333333333\n",
      "Test accuracy is: 0.8191666666666667\n",
      "Train accuracy is: 0.7792083333333333\n",
      "Test accuracy is: 0.7771666666666667\n",
      "Train accuracy is: 0.7792083333333333\n",
      "Test accuracy is: 0.7771666666666667\n",
      "Train accuracy is: 0.7792083333333333\n",
      "Test accuracy is: 0.7771666666666667\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Score:   0.6620726732118593\n",
      "Best Params:  {'ccp_alpha': 0.0001, 'max_depth': 5}\n",
      "Train accuracy is: 0.8220833333333334\n",
      "Test accuracy is: 0.8198333333333333\n"
     ]
    }
   ],
   "source": [
    "# Growing a collection of Trees (Doing 4 Trees for now)\n",
    "clf_tree = DecisionTreeClassifier(max_depth=5, ccp_alpha=0.00001).fit(X_train, y_train)\n",
    "y_pred = clf_tree.predict(X_test)\n",
    "print('Train accuracy is: ' + str(accuracy_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test accuracy is: ' + str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(max_depth=50, ccp_alpha=0.1).fit(X_train, y_train)\n",
    "y_pred = clf_tree.predict(X_test)\n",
    "print('Train accuracy is: ' + str(accuracy_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test accuracy is: ' + str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(max_depth=10, ccp_alpha=1).fit(X_train, y_train)\n",
    "y_pred = clf_tree.predict(X_test)\n",
    "print('Train accuracy is: ' + str(accuracy_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test accuracy is: ' + str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(max_depth=50, ccp_alpha=0.1).fit(X_train, y_train)\n",
    "y_pred = clf_tree.predict(X_test)\n",
    "print('Train accuracy is: ' + str(accuracy_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test accuracy is: ' + str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "# Seeing what parameters actually work well with Grid Search\n",
    "param_grid = dict(max_depth=[10, 20, 30],\n",
    "                  ccp_alpha=[0.00001,0.0001, 0.001],\n",
    "                    min_samples_leaf=[])\n",
    "\n",
    "# Define classifier\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "# Scoring criterion\n",
    "roc = make_scorer(roc_auc_score)\n",
    "\n",
    "# Searching for best with grid search\n",
    "grid = GridSearchCV(estimator=clf_tree,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring=roc,\n",
    "                    verbose=1,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid_result_tree = grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score:  ', grid_result_tree.best_score_)\n",
    "print('Best Params: ', grid_result_tree.best_params_)\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(**grid_result_tree.best_params_).fit(X_train, y_train)\n",
    "\n",
    "# We are comparing y_test vs y_pred here\n",
    "y_pred = clf_tree.predict(X_test)\n",
    "print('Train accuracy is: ' + str(accuracy_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test accuracy is: ' + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d323d",
   "metadata": {},
   "source": [
    "According to the results, the best parameters for the formation of the decisions trees was when ccp_alpha was 1e-4, and min_samples_leaf was 20. The ccp_alpha optimal value was relatively close-ish to zero: which means that a smaller portion of the tree was pruned, leading to a decision tree that generalizes a bit on the less side, which could be beneficial for the given distribution of the data. The most optimal max_depth seemed to be 5, which is on the less side: which could make sense given our data if it was in a more clustered distribution, therefore, it would be better not to make very deep decision trees with lots of splits, as it could lead to overfitting. As for the test trees we did, it seemed that the trees with low depth and low ccp did the best, which leads to a somewhat general split, which could be good for the distrubtion of our data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b842d",
   "metadata": {},
   "source": [
    "(ii) Find the maximum tree depth and pruning parameter that maximise the recall instead. Provide some intuition for why we may want to consider recall in place of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a129b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best Score:   0.3793204696491885\n",
      "Best Params:  {'ccp_alpha': 1e-05, 'min_samples_leaf': 20}\n",
      "Train recall is: 0.382902434421589\n",
      "Test recall is: 0.3679880329094989\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(min_samples_leaf=[5, 10, 20],\n",
    "                  ccp_alpha=[0,0.00001,0.0001])\n",
    "\n",
    "# Define classifier\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Searching for best with grid search\n",
    "grid = GridSearchCV(estimator=clf_tree,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring=\"recall\",\n",
    "                    verbose=1,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid_result_tree = grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score:  ', grid_result_tree.best_score_)\n",
    "print('Best Params: ', grid_result_tree.best_params_)\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(**grid_result_tree.best_params_).fit(X_train, y_train)\n",
    "\n",
    "# We are comparing y_test vs y_pred here\n",
    "y_pred = clf_tree.predict(X_test)\n",
    "print('Train recall is: ' + str(recall_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test recall is: ' + str(recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d50f81",
   "metadata": {},
   "source": [
    "We may want to consider recall instead of accuracy because sometimes we want to value each outcome equally instead of focusing on one type of outcome over another: where both false negatives and true positives must be equally regarded. In this situation, it would make more sense to use recall, since we really really want to find people who will default, even at the expense of classifying more people as willing to default. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb583c8",
   "metadata": {},
   "source": [
    "(iii) Explain the difference between a random forest and a decision tree.\n",
    "\n",
    "Although both are types of trees, it uses different algorithms to generate the trees. \n",
    "A decision tree usually implements a more greedy heuristic to find the most optimal decision tree by making locally optimized decisions: it looks for the split at each node that minimizes the classification error rate (fraction of training observations in a given region that doesn't belong to the most common class).\n",
    "A random tree is a more optimized algorithm offers the potential to make seemingly useless splits early on, but very useful splits later. It takes a random sample of m out of the total n predictors, and only uses those for splitting in a similar process to the decision tree process described above. (Forces tree to consider otherwise seemingly pointless predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbfd30",
   "metadata": {},
   "source": [
    "Using an aggregation technique (such as random forest or boosting), attempt to optimize the classification performance of your model. You may use any skills/tricks that you believe may help so long as the basic model remains a decision tree. Plot the ROC curve and calculate the area under the ROC curve for your “optimized” model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b30307f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best Score:   0.6622216654539967\n",
      "Best Params:  {'learning_rate': 0.2, 'n_estimators': 1000}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_grid = dict(n_estimators=[500, 1000, 1500],\n",
    "                  learning_rate=[0.05, 0.1, 0.2]\n",
    "             )\n",
    "\n",
    "# Define classifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "clf_tree = AdaBoostClassifier(base_estimator = dtc)\n",
    "# Scoring criterion\n",
    "roc = make_scorer(roc_auc_score)\n",
    "\n",
    "# Searching for best with grid search\n",
    "grid = GridSearchCV(estimator=clf_tree,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring=roc,\n",
    "                    verbose=1,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid_result_tree = grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score:  ', grid_result_tree.best_score_)\n",
    "print('Best Params: ', grid_result_tree.best_params_)\n",
    "\n",
    "# Plot the ROC and compare\n",
    "clf_tree = AdaBoostClassifier(**grid_result_tree.best_params_).fit(X_train, y_train)\n",
    "y_pred_tree = clf_tree.predict_proba(X_test)\n",
    "fpr_tree, tpr_tree, _ = roc_curve(y_test, y_pred_tree[:,1])\n",
    "auc_tree = auc(fpr_tree, tpr_tree)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], '--')\n",
    "plt.plot(fpr_tree, tpr_tree, label='Tree (area = {:.3f})'.format(auc_tree))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087873ab",
   "metadata": {},
   "source": [
    "(c) Compare the performance of the logistic regression (from homework 1), decision tree models (including single tree model, random forests, and other tree based models), and SVM in terms of accuracy/precision/recall and ROC. Briefly comment on the differences that you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e32f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7334143244186999\n",
      "Train accuracy is: 0.8209583333333333\n",
      "Test accuracy is: 0.82\n",
      "Train recall is: 0.36270994527269296\n",
      "Test recall is: 0.3537771129394166\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the accuracy, recall and AUC ROC of the logistic regression: (precision is omitted due to low relevance)\n",
    "clf_logit = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = clf_tree.predict(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_tree[:,1])\n",
    "auc_logit = auc(fpr, tpr)\n",
    "print(auc_logit)\n",
    "\n",
    "print('Train accuracy is: ' + str(accuracy_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test accuracy is: ' + str(accuracy_score(y_test, y_pred)))\n",
    "print('Train recall is: ' + str(recall_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test recall is: ' + str(recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debac1a5",
   "metadata": {},
   "source": [
    "In comparison to the logistic regression model on this data set, The train and test accuracy of the single tree model, SVM, and the boosted model all performed relatively similarly. I believe the reason for this is because I omitted alot of the data due to run time issues, and only focused on the 5 most important variables. Since the 5 most correlated variables happened to all be the history of past payment and are all similar to each other, this means that no matter how the variety of classification methods tried to optimize, the lack of variation in the predictor variables made it difficult to optimize past a threshold point, which can be seen as most of the optimized values of AUC ROC, accuracy, and even recall converge to around the same value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9eb255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
