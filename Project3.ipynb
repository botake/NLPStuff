{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4a2cd9",
   "metadata": {},
   "source": [
    "1. Default prediction: Method Comparison\n",
    "\n",
    "This question asks you to continue comparing the predictive accuracy of different classification\n",
    "methods for predicting default. You do not need to write the code from scratch. Instead, you are\n",
    "allowed to use statistical and machine learning packages.\n",
    "\n",
    "As in homework 1 and 2, the data for this question can be found in the file default_data.xls.\n",
    "It contains data on customers’ default payments in Taiwan. We will use the area under the ROC\n",
    "curve (abbreviated to ROC AUC) to compare across the techniques.\n",
    "\n",
    "(a) (Neural Networks). Now, use a neural network approach to predict default:\n",
    "•Fit the neural network using ReLU and Sigmoid functions. For each activation function,\n",
    "fit a collection of neural networks by varying (i) the number of hidden layers, (ii) the\n",
    "number of nodes in the layers, and (iii) the drop-out rate.\n",
    "\n",
    "•Compare the ROC AUC for the training data and the ROC AUC for the test data for\n",
    "each neural network you fitted in the previous step. Provide some intuition for your\n",
    "results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee44742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcfa2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Creating neural net models varying number of hidden layer, number of nodes, and the dropout rate\\nmodels = []\\n# base \\nmodel1 = keras.models.Sequential([\\n  tf.keras.layers.Dense(32, activation='relu'),\\n  tf.keras.layers.Dropout(0.2),  \\n  tf.keras.layers.Dense(16, activation='relu'),\\n  tf.keras.layers.Dropout(0.2),\\n  tf.keras.layers.Dense(8, activation='sigmoid'),\\n  tf.keras.layers.Dropout(0.2),  \\n  tf.keras.layers.Dense(2, activation='softmax')\\n])\\nmodels.append(model1)\\n\\n# one less hidden layer\\nmodel2 = keras.models.Sequential([ \\n  tf.keras.layers.Dense(16, activation='relu'),\\n  tf.keras.layers.Dropout(0.2),\\n  tf.keras.layers.Dense(8, activation='sigmoid'),\\n  tf.keras.layers.Dropout(0.2),  \\n  tf.keras.layers.Dense(2, activation='softmax')\\n])\\nmodels.append(model2)\\n\\n# different number of nodes\\nmodel3 = keras.models.Sequential([\\n  tf.keras.layers.Dense(20, activation='relu'),\\n  tf.keras.layers.Dropout(0.2),  \\n  tf.keras.layers.Dense(10, activation='relu'),\\n  tf.keras.layers.Dropout(0.2),\\n  tf.keras.layers.Dense(6, activation='sigmoid'),\\n  tf.keras.layers.Dropout(0.2),  \\n  tf.keras.layers.Dense(2, activation='softmax')\\n])\\nmodels.append(model3)\\n\\n# different dropout rate\\nmodel4 = keras.models.Sequential([\\n  tf.keras.layers.Dense(32, activation='relu'),\\n  tf.keras.layers.Dropout(0.3),  \\n  tf.keras.layers.Dense(16, activation='relu'),\\n  tf.keras.layers.Dropout(0.3),\\n  tf.keras.layers.Dense(8, activation='sigmoid'),\\n  tf.keras.layers.Dropout(0.3),  \\n  tf.keras.layers.Dense(2, activation='softmax')\\n])\\nmodels.append(model4)\\n\\n# mixed\\nmodel5 = keras.models.Sequential([\\n  tf.keras.layers.Dense(50, activation='relu'),\\n  tf.keras.layers.Dropout(0.1),  \\n  tf.keras.layers.Dense(30, activation='relu'),\\n  tf.keras.layers.Dropout(0.1),\\n  tf.keras.layers.Dense(10, activation='sigmoid'),\\n  tf.keras.layers.Dropout(0.1),  \\n  tf.keras.layers.Dense(2, activation='softmax')\\n])\\nmodels.append(model5)\\n\\n# Compiling and fitting models\\nfor model in models:\\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\\n    model.fit(X_train, y_train.to_numpy(), epochs=5, batch_size=32)\\n\\n# Comparing ROC AUC for training and test data for each neural network\\nfpr_nn = []\\ntpr_nn = []\\nauc_nn = []\\nfor model in models:\\n    y_pred_nn = model.predict(X_test)\\n    fpr_nn_temp, tpr_nn_temp, thresholds_nn_temp = roc_curve(y_test, y_pred_nn[:,1]) \\n    auc_nn_temp = auc(fpr_nn_temp, tpr_nn_temp) \\n    fpr_nn.append(fpr_nn_temp)\\n    tpr_nn.append(tpr_nn_temp)\\n    auc_nn.append(auc_nn_temp)\\n    \\n# Plot the ROC and compare\\nplt.figure(1)\\nplt.plot([0, 1], [0, 1], '--')\\n\\nfor i in range(5):\\n    plt.plot(fpr_nn[i], tpr_nn[i], label='NN (area = {:.3f})'.format(auc_nn))\\n\\nplt.xlabel('False positive rate')\\nplt.ylabel('True positive rate')\\nplt.title('ROC curve')\\nplt.legend(loc='best')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r'/Users/benshi/Downloads/default_data.xls').drop([0])\n",
    "df = df.iloc[:,1:]\n",
    "\n",
    "# Specify target and covariate data\n",
    "X = df.drop(columns=['Y'])\n",
    "y = df['Y']\n",
    "\n",
    "# Replace empty with column mean\n",
    "X = X.fillna(X.mean()) \n",
    "\n",
    "# Clip the data at 2% tails\n",
    "X = X.clip(lower=X.quantile(0.02), upper=X.quantile(0.98), axis=1)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into 80/20 train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Setting as float\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "# Classes in data\n",
    "class_names=['No Default', 'Default']\n",
    "\n",
    "\"\"\"\n",
    "# Creating neural net models varying number of hidden layer, number of nodes, and the dropout rate\n",
    "models = []\n",
    "# base \n",
    "model1 = keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),  \n",
    "  tf.keras.layers.Dense(16, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(8, activation='sigmoid'),\n",
    "  tf.keras.layers.Dropout(0.2),  \n",
    "  tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "models.append(model1)\n",
    "\n",
    "# one less hidden layer\n",
    "model2 = keras.models.Sequential([ \n",
    "  tf.keras.layers.Dense(16, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(8, activation='sigmoid'),\n",
    "  tf.keras.layers.Dropout(0.2),  \n",
    "  tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "models.append(model2)\n",
    "\n",
    "# different number of nodes\n",
    "model3 = keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(20, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),  \n",
    "  tf.keras.layers.Dense(10, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(6, activation='sigmoid'),\n",
    "  tf.keras.layers.Dropout(0.2),  \n",
    "  tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "models.append(model3)\n",
    "\n",
    "# different dropout rate\n",
    "model4 = keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.3),  \n",
    "  tf.keras.layers.Dense(16, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(8, activation='sigmoid'),\n",
    "  tf.keras.layers.Dropout(0.3),  \n",
    "  tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "models.append(model4)\n",
    "\n",
    "# mixed\n",
    "model5 = keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(50, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.1),  \n",
    "  tf.keras.layers.Dense(30, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.1),\n",
    "  tf.keras.layers.Dense(10, activation='sigmoid'),\n",
    "  tf.keras.layers.Dropout(0.1),  \n",
    "  tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "models.append(model5)\n",
    "\n",
    "# Compiling and fitting models\n",
    "for model in models:\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train.to_numpy(), epochs=5, batch_size=32)\n",
    "\n",
    "# Comparing ROC AUC for training and test data for each neural network\n",
    "fpr_nn = []\n",
    "tpr_nn = []\n",
    "auc_nn = []\n",
    "for model in models:\n",
    "    y_pred_nn = model.predict(X_test)\n",
    "    fpr_nn_temp, tpr_nn_temp, thresholds_nn_temp = roc_curve(y_test, y_pred_nn[:,1]) \n",
    "    auc_nn_temp = auc(fpr_nn_temp, tpr_nn_temp) \n",
    "    fpr_nn.append(fpr_nn_temp)\n",
    "    tpr_nn.append(tpr_nn_temp)\n",
    "    auc_nn.append(auc_nn_temp)\n",
    "    \n",
    "# Plot the ROC and compare\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], '--')\n",
    "\n",
    "for i in range(5):\n",
    "    plt.plot(fpr_nn[i], tpr_nn[i], label='NN (area = {:.3f})'.format(auc_nn))\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfb7003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Score for linear kernel:   0.7800416666666666\n",
      "Best Params for linear kernel:  {'C': 1, 'class_weight': 'balanced', 'kernel': 'poly', 'max_iter': 100000000.0}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d3be0ce625a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mclf_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mgrid_result_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0my_pred_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mfpr_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_svm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mauc_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_svm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \"\"\"\n\u001b[0;32m--> 666\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[1;32m    634\u001b[0m                                  \" probability=False\")\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'c_svc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nu_svc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "param_grid = dict(C=[1, 10],\n",
    "                  kernel=['linear', 'poly', 'rbf'],\n",
    "                  max_iter=[1e8],\n",
    "                  class_weight=['balanced'])\n",
    "clf_svm = SVC()\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "grid_svm = GridSearchCV(estimator=clf_svm,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring=accuracy,\n",
    "                    verbose=1,\n",
    "                    cv=5,\n",
    "                    n_jobs=1)\n",
    "grid_result_svm = grid_svm.fit(X_train, y_train)\n",
    "print('Best Score for SVM:  ', grid_result_svm.best_score_)\n",
    "print('Best Params for SVM: ', grid_result_svm.best_params_)\n",
    "\n",
    "clf_svm = SVC(**grid_result_svm.best_params_).fit(X_train, y_train)\n",
    "y_pred_svm = clf_svm.predict_proba(X_test)\n",
    "fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_pred_svm)\n",
    "auc_svm = auc(fpr_svm, tpr_svm)\n",
    "\n",
    "print('SVM:')\n",
    "print('SVM AUC: ' + str(auc_svm))\n",
    "print('Train accuracy is: ' + str(accuracy_score(y_train, clf_svm.predict(X_train))))\n",
    "print('Test accuracy is: ' + str(accuracy_score(y_test, y_pred_svm)))\n",
    "print('Train recall is: ' + str(recall_score(y_train, clf_svm.predict(X_train))))\n",
    "print('Test recall is: ' + str(recall_score(y_test, y_pred_svm)))\n",
    "print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f9633",
   "metadata": {},
   "source": [
    "(b) Compare the performance of the logistic regression (from homework 1), decision tree models\n",
    "(from homework 1), SVM and neural networks in terms of accuracy/precision/recall and\n",
    "ROC. Briefly comment on the differences that you find. (HINT 1: You should assume that\n",
    "you should use a threshold of 0.5 whenever ambiguous.) (HINT 2: It is difficult to specify\n",
    "a CV score criterion for Keras. Thus, you should use accuracy (the default in Keras) for\n",
    "all classifiers, i.e. during the GridSearchCV use scoring = accuracy for logistic regression,\n",
    "decision trees, and SVMs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "clf_logit = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred_logit = clf_tree.predict(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_tree[:,1])\n",
    "auc_logit = auc(fpr, tpr)\n",
    "\n",
    "print('Logistic Regression:')\n",
    "print('Logistic Regression AUC: ' + str(auc_logit))\n",
    "print('Train accuracy is: ' + str(accuracy_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test accuracy is: ' + str(accuracy_score(y_test, y_pred_logit)))\n",
    "print('Train recall is: ' + str(recall_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test recall is: ' + str(recall_score(y_test, y_pred_logit)))\n",
    "\n",
    "# Decision Tree Models\n",
    "clf_tree = DecisionTreeClassifier(max_depth=5, ccp_alpha=0.00001).fit(X_train, y_train)\n",
    "y_pred_tree = clf_tree.predict(X_test)\n",
    "fpr_tree, tpr_tree, _ = roc_curve(y_test, y_pred_tree[:,1])\n",
    "auc_tree = auc(fpr_tree, tpr_tree)\n",
    "\n",
    "print('Decision Tree:')\n",
    "print('Decision Tree AUC: ' + str(auc_tree))\n",
    "print('Train accuracy is: ' + str(accuracy_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test accuracy is: ' + str(accuracy_score(y_test, y_pred_tree)))\n",
    "print('Train recall is: ' + str(recall_score(y_train, clf_tree.predict(X_train))))\n",
    "print('Test recall is: ' + str(recall_score(y_test, y_pred_tree)))\n",
    "\n",
    "# SVM: (Use optimal parameters from previous homeworks)\n",
    "svm_params = {'C': 100, 'kernel': 'poly', 'max_iter': 100000.0,\n",
    "            'probability': True, 'class_weight': 'balanced'}\n",
    "clf_svm = SVC(**svm_params).fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = clf_svm.predict_proba(X_test)\n",
    "fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_pred_svm[:,1])\n",
    "auc_svm = auc(fpr_svm, tpr_svm)\n",
    "\n",
    "print('SVM:')\n",
    "print('SVM AUC: ' + str(auc_svm))\n",
    "print('Train accuracy is: ' + str(accuracy_score(y_train, clf_svm.predict(X_train))))\n",
    "print('Test accuracy is: ' + str(accuracy_score(y_test, y_pred_svm)))\n",
    "print('Train recall is: ' + str(recall_score(y_train, clf_svm.predict(X_train))))\n",
    "print('Test recall is: ' + str(recall_score(y_test, y_pred_svm)))\n",
    "\n",
    "# Neural Networks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5750d",
   "metadata": {},
   "source": [
    "2 Estimating Interest Rates\n",
    "\n",
    "This question asks you to use estimate the interest rates charged for AI and non-AI lenders.\n",
    "The data for parts (a) and (b) can be found in the file lendclub_data.csv. Use lendclub_data_\n",
    "test.csv for parts (c) and (d). It contains data on characteristics and default outcomes for lending\n",
    "club loans. Before you proceed, you need to convert the categorical variables properly.\n",
    "\n",
    "Clarification: The “group” variable is artificially generated and should be considered a “restricted”\n",
    "variable, such as race or gender. By this, we meant that the group variable should not enter your\n",
    "prediction algorithms, but should only be used ex-post for distribution analysis in part (e).\n",
    "\n",
    "(a) (Logistic Regression). Use the Logistic Regression to estimate the probability default for\n",
    "borrowers with different characteristics. Report the average AUC ROC using 5-fold cross-\n",
    "validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7763b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(r'/Users/benshi/Downloads/lendingclub_data.csv')\n",
    "\n",
    "X = df_data.drop(['charged_off', 'groups'], axis=1)\n",
    "y = df_data['charged_off']\n",
    "\n",
    "# Converting categorical variables:\n",
    "X = pd.get_dummies(X, columns=['sub_grade', 'home_ownership',\n",
    "                               'purpose', 'application_type'], drop_first=True)\n",
    "\n",
    "# Normalize numerical variables:\n",
    "var_num = ['loan_amnt', 'term', 'int_rate', 'installment', 'emp_length', 'open_acc', \n",
    "           'revol_util', 'total_acc', 'dti', 'log_annual_inc', 'fico_score', 'log_revol_bal']\n",
    "\n",
    "X_num = X[var_num]\n",
    "X_cat = X.drop(columns=var_num, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num_sc = pd.DataFrame(scaler.fit_transform(X_num))\n",
    "X_num_sc.columns = X_num.columns\n",
    "X_sc = X_num_sc.join(X_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e44c2a",
   "metadata": {},
   "source": [
    "(b) (Gradient Boosting). Use the GradientBoostingClassifier from sklearn to estimate the proba-\n",
    "bility default for borrowers with different characteristics. Tune the hyperparameters covered\n",
    "in precept (e.g. n_estimators, max_depth, etc.) to achieve a better average 5-fold cross-\n",
    "validation AUC ROC. Report the average AUC ROC using 5-fold cross-validation and the\n",
    "resulting hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b33df0",
   "metadata": {},
   "source": [
    "(c) Compare the predicted default probabilities, i.e. second column of clf.predict_proba(X_test),\n",
    "between the methods. What happens to the dispersion of probabilities? Notice that you\n",
    "should use the test data to do this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3402853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_test = pd.read_csv(r'/Users/benshi/Downloads/lendingclub_data_test.csv')\n",
    "\n",
    "X_test = df_data.drop(['charged_off', 'groups'], axis=1)\n",
    "y_test = df_data['charged_off']\n",
    "\n",
    "# Converting categorical variables:\n",
    "X_test = pd.get_dummies(X_test, columns=['sub_grade', 'home_ownership',\n",
    "                               'purpose', 'application_type'], drop_first=True)\n",
    "\n",
    "# Normalize numerical variables:\n",
    "var_num = ['loan_amnt', 'term', 'int_rate', 'installment', 'emp_length', 'open_acc', \n",
    "           'revol_util', 'total_acc', 'dti', 'log_annual_inc', 'fico_score', 'log_revol_bal']\n",
    "\n",
    "X_test_num = X_test[var_num]\n",
    "X_test_cat = X_test.drop(columns=var_num, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_test_num_sc = pd.DataFrame(scaler.fit_transform(X_test_num))\n",
    "X_test_num_sc.columns = X_test_num.columns\n",
    "X_test_sc = X_test_num_sc.join(X_cat)\n",
    "\n",
    "# Training the models \n",
    "clf_gb = GradientBoostingClassifier(**grid_result.best_params_).fit(X_sc, y)\n",
    "\n",
    "# Predicting\n",
    "clf_gb.predict_proba(X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58507130",
   "metadata": {},
   "source": [
    "(d) The equilibrium condition is given by:\n",
    "\n",
    "Eq.1\n",
    "\n",
    "where we changed the α in lec08b.pdf to a fixed value LGD (loss given default) for simplicity.\n",
    "Using the test data and the two different classifiers found in parts (a) and (b):\n",
    "\n",
    "(i) Compute the equilibrium interest rate r∗ which solves the above for each different indi-\n",
    "vidual xi, and\n",
    "\n",
    "(ii) Record whether fsolve found such solution r∗, i.e. whether the loan was accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "# Define constants\n",
    "rho = 0.04\n",
    "LGD = 0.5\n",
    "\n",
    "def solve_r(r, x):\n",
    "    x_new = x.copy()\n",
    "    \n",
    "    # the third variable is the interest rate; we replace with new interest rate        \n",
    "    x_new[2] = r\n",
    "    x_new = x_new.to_numpy().reshape(1, -1)\n",
    "    \n",
    "    # use the predicted probability given initial features + new interest rate\n",
    "    phat = clf_gb.predict_proba(x_new)\n",
    "    phat = phat[0][1]\n",
    "    \n",
    "    # NPV function here\n",
    "    F = (1/(1+rho)) * ( (1-phat)*(1+r) + phat*(1-LGD) ) - 1\n",
    "\n",
    "    return F\n",
    "\n",
    "fsolve(solve_r, 0, args=X_test_sc, xtol=1e-4, full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe6827",
   "metadata": {},
   "source": [
    "(e) At this point, you should have a list of equilibrium interest rates and a list of convergence\n",
    "results for both Logistic Regression and Gradient Boosting. The test dataset also includes\n",
    "an extra “group” variable that is artificially generated. Discuss how the distribution of equi-\n",
    "librium interest rates and loan acceptance decisions among the different “groups” change as\n",
    "we move from Logistic Regression to Gradient Boosting. Who gains and who loses from this\n",
    "move to a more complex algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096cdd59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b66272a4",
   "metadata": {},
   "source": [
    "3 Cryptocurrency Historical Prices\n",
    "\n",
    "This question asks you to investigate the correlation between cryptocurrency daily returns and\n",
    "other assets\n",
    "\n",
    "(a) Download cryptocurrency price data (for tickers BTC, ETH, LTC, BCH, ETC, EOS, LINK,\n",
    "REP, XLM, and XRP) from the coinbase website (URL: https://www.cryptodatadownload.\n",
    "com/data/coinbase/) or another cryptocurrency data website.\n",
    "\n",
    "(i) On one graph, plot price indices for all cryptocurrencies that were trading on 1st January\n",
    "2017, with the price indices indexed to 100 on 1st January 2017. Your plot should cover\n",
    "the period between 1st January 2017 to the most recent available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a260f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the datasets (Already trimmed to start from Jan 2017)\n",
    "df_BTC = pd.read_csv(r'/Users/benshi/Downloads/cryptoPrices/BTC-USD.csv')\n",
    "df_ETH = pd.read_csv(r'/Users/benshi/Downloads/cryptoPrices/ETH-USD.csv')\n",
    "df_LTC = pd.read_csv(r'/Users/benshi/Downloads/cryptoPrices/LTC-USD.csv')\n",
    "df_BCH = pd.read_csv(r'/Users/benshi/Downloads/cryptoPrices/BCH-USD.csv')\n",
    "df_ETC = pd.read_csv(r'/Users/benshi/Downloads/cryptoPrices/ETC-USD.csv')\n",
    "df_EOS = pd.read_csv(r'/Users/benshi/Downloads/cryptoPrices/EOS-USD.csv')\n",
    "df_LINK = pd.read_csv(r'/Users/benshi/Downloads/cryptoPrices/LINK-USD.csv')\n",
    "df_REP = pd.read_csv(r'/Users/benshi/Downloads/cryptoPrices/REP-BTC.csv')\n",
    "df_XLM = pd.read_csv(r'/Users/benshi/Downloads/cryptoPrices/XLM-USD.csv')\n",
    "df_XRP = pd.read_csv(r'/Users/benshi/Downloads/cryptoPrices/XRP-USD.csv')\n",
    "\n",
    "# Cleaning/Merging and plotting the dataset\n",
    "df_BTC = df_BTC.set_index('date')\n",
    "df_BTC['Close'] = 100 * df_BTC['Close'] / df_BTC.loc[1, 'Close']\n",
    "df = df_BTC['Close'].rename(columns={'Close': 'btc'})\n",
    "\n",
    "\n",
    "df = df.dropna()\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6101c3",
   "metadata": {},
   "source": [
    "(ii) What annualized return would you received if you purchased Bitcoin on 1st January\n",
    "2017 and sold on 1st October 2021? What about Ethereum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized return for bitcoin:\n",
    "initial = df_BTC['2017-01-01', 'Close'] \n",
    "final = df_BTC['2021-10-01', 'Close']\n",
    "ann_return = (final - initial) / initial\n",
    "print('Annualized return for bitcoin is: ' + str(ann_return))\n",
    "\n",
    "# Annualized return for Ethereum:\n",
    "initial = df_ETH['2017-01-01', 'Close'] \n",
    "final = df_ETH['2021-10-01', 'Close']\n",
    "ann_return = (final - initial) / initial\n",
    "print('Annualized return for bitcoin is: ' + str(ann_return))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44385e4",
   "metadata": {},
   "source": [
    "(iii) Calculate daily returns (i.e. daily percentage changes in the price) for each cryptocur-\n",
    "rency (i.e. for tickers BTC, ETH, LTC, BCH, ETC, EOS, LINK, REP, XLM, and XRP).\n",
    "Report the standard deviation of daily returns for each cryptocurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05677754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bc1d2ce",
   "metadata": {},
   "source": [
    "(b) Download data on the Gold Fixing Price 10:30 A.M. (e.g. from FRED https://fred.\n",
    "stlouisfed.org/series/GOLDAMGBD228NLBM), the 1-year US treasury yield (e.g. from FRED\n",
    "https://fred.stlouisfed.org/series/DGS1), the 10-year US treasury yield (e.g. from\n",
    "FRED https://fred.stlouisfed.org/series/DGS10), the S&P 500 (e.g. from FRED https:\n",
    "//fred.stlouisfed.org/series/SP500),\n",
    "\n",
    "(i) On one graph, plot price indices for Bitcoin, gold, and the S&P 500, with price indices\n",
    "indexed to 100 on 1st January 2017. Your plot should cover the period between 1st\n",
    "January 2017 to the most recent available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280d063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8336f6e",
   "metadata": {},
   "source": [
    "(ii) Calculate daily returns for gold and the S&P 500 for each day in the period from 1st\n",
    "January 2015 to 1st October 2021. What is the standard deviation of daily returns for\n",
    "gold and the S&P 500? What is correlation between daily Bitcoin returns and daily\n",
    "gold returns over this period. What is the correlation between daily Bitcoin returns and\n",
    "daily S&P returns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407628e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3b85b83",
   "metadata": {},
   "source": [
    "(iii) Calculate the rolling monthly correlation between Bitcoin daily returns and gold daily\n",
    "returns at each date between 1st January 2015 and 1st October 2021. Calculate the\n",
    "rolling monthly correlation between Bitcoin daily returns and S&P 500 daily returns at\n",
    "each date between 1st January 2015 and 1st October 2021. Plot both correlation series.\n",
    "Have the correlations changed over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ef88fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06465d46",
   "metadata": {},
   "source": [
    "(iv) Repeat the analysis from the previous question but now consider the correlation between\n",
    "Bitcoin daily returns and the 1-year US treasury yield and the 10-year US treasury yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc870676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
